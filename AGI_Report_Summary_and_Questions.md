# 自研 AGI 报告摘要与待研究问题清单

> 基于原始文档《自研AGI报告.docx》提炼。

## 第一部分：报告核心摘要

本报告的核心论点是：**在 2025 年的消费级硬件条件下，通过“系统架构创新”而非“模型参数堆叠”，可以构建出具备 AGI 特质（自适应、长期记忆、自我改进）的智能体系统。**

### 1. 核心战略：从“炼大模型”转向“构建心智系统”
*   **不可行路径**：在个人电脑上从零预训练（Pre-training）一个基座大模型。
*   **可行路径**：利用开源的高质量基座（如 Qwen 2.5 32B），外挂一个复杂的认知架构（Cognitive Architecture）。
*   **关键隐喻**：基座模型只是“大脑皮层”的一小块切片，AGI 需要的是包含海马体（记忆）、前额叶（规划）和睡眠机制（整理）的完整操作系统。

### 2. 硬件基础设施 ("Cortex-1" 工作站)
*   **算力**：推荐双路 RTX 3090/4090 (24GB x2)，通过 NVLink 或 PCIe 拆分构建 48GB 显存池。
*   **存储**：高速 NVMe SSD 用于向量库读写；大容量 RAM (128GB+) 用于将非计算密集任务卸载到 CPU。
*   **瓶颈**：显存带宽（Memory Wall）是推理速度的决定性因素，而非计算核心数。

### 3. 软件与算法架构
*   **执行核心**：Qwen 2.5 32B Instruct (Int4 量化)，运行于 vLLM 或 llama.cpp。
*   **认知框架**：
    *   **LangGraph**：实现有状态、可循环的图式编排（思考->执行->反思）。
    *   **多智能体 (MoA)**：Planner（规划）、Executor（执行）、Critic（审查）的分工协作。
*   **记忆系统**：
    *   **RAG**：基于向量数据库的短期/中期记忆。
    *   **知识图谱 (KG)**：用于存储结构化事实，解决逻辑一致性问题。
*   **进化机制 (昼夜节律)**：
    *   **Day Mode**：实时交互，收集经验（Logs）。
    *   **Night Mode**：离线“睡眠”，进行记忆压缩、遗忘与模型微调（QLoRA/Unsloth）。

### 4. 前沿/边缘探索 (The Fringe)
*   报告提及了一些极具潜力但尚未成熟的技术：**BitNet (1.58-bit)** 极低比特网络、**超维计算 (HDC)**、**哥德尔机 (Gödel Machine)** 自我重写代码。

---

## 第二部分：待研究问题清单 (Research Gaps)

基于报告中提出的设想与现状的差距，以下是需要进一步攻关或验证的“深水区”问题：

### 1. 算法与模型层 (Algorithm Layer)
- [ ] **BitNet 的工程落地**：
    *   *问题*：目前缺乏成熟的 1.58-bit 推理框架。如何在不损失指令遵循能力的前提下，将现有的 Llama/Qwen 模型蒸馏或量化为 1.58-bit？
- [ ] **无限上下文的真实效果**：
    *   *问题*：Ring Attention 和 Infini-attention 在理论上支持无限窗口，但在单机有限显存下，如何避免 "Lost-in-the-middle"（中间迷失）现象？
- [ ] **超维计算 (HDC) 的融合**：
    *   *问题*：如何将 HDC 的全息向量运算与 Transformer 的 Embedding 空间对齐？它能否真正替代向量数据库作为更高效的“联想记忆”？

### 2. 认知架构层 (Cognitive Architecture)
- [ ] **记忆的“睡眠固化”算法**：
    *   *问题*：在“夜晚模式”中，依据什么标准判定一条短期记忆应该被“遗忘”、被“存入向量库”还是被“微调进模型权重”？需要一个具体的价值评估函数。
- [ ] **神经符号 (Neuro-Symbolic) 的接口**：
    *   *问题*：如何让 LLM 稳定地将自然语言转化为知识图谱（Neo4j）的 Cypher 查询语句，且不因 schema 变更而崩溃？
- [ ] **OODA 循环的延迟控制**：
    *   *问题*：引入多智能体辩论（Planner-Critic loop）会显著增加响应延迟。如何在“快思考”（直觉反应）和“慢思考”（深度规划）之间动态切换？

### 3. 自我进化与安全 (Self-Improvement & Safety)
- [ ] **哥德尔智能体的“死锁”风险**：
    *   *问题*：如果 Agent 修改了自己的代码导致系统崩溃或死循环，如何设计一个独立于 Agent 之外的“看门狗”机制来回滚状态？
- [ ] **微调的灾难性遗忘**：
    *   *问题*：在每日夜间微调（Continuous Pre-training/Fine-tuning）中，如何确保新学到的知识不会覆盖掉通用的逻辑推理能力？（需要构建本地的回归测试集）。

### 4. 硬件工程层 (Hardware Engineering)
- [ ] **双卡互联效率**：
    *   *问题*：在消费级主板上，PCIe 通道拆分（x8/x8）对推理和微调速度的实际损耗是多少？是否必须上 NVLink？
- [ ] **异构计算卸载**：
    *   *问题*：如何高效利用 CPU RAM 暂存 KV Cache，以换取更大的上下文窗口，同时保持可接受的 token 生成速度？

---

## 建议的行动优先级

1.  **P0**: 解决 **“记忆固化算法”** —— 这是区分普通 RAG 助手与成长型 AGI 的关键。
2.  **P1**: 建立 **“回归测试集”** —— 没有这个，任何自我进化都是盲人摸象，极易导致模型退化。
3.  **P2**: 探索 **“混合精度/量化”** —— 压榨 RTX 3090/4090 的极限，通过工程手段换取参数量。
